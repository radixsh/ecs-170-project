import logging
import os
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
import time
from custom_functions import *
from env import CONFIG, NUM_DIMENSIONS

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
console_handler = logging.StreamHandler()
logger.addHandler(console_handler)

### Other stuff

def generate_data(count, sample_size):
    data = []
    # Generate 'count' examples uniformly at random (before, it was 9 * 'count')
    for _ in range(count):
        dist_class = np.random.choice(DISTRIBUTIONS.items())
        dist_object = dist_class()
        points = dist_object.rng(sample_size)
        label = dist_object.get_label()
        data.append((points, label))
        # items = list(DISTRIBUTIONS.items())
        # choice = np.random.choice(len(items))
        # _, dist_func = items[choice]
        # points, labels = dist_func(sample_size)
        # data.append((points, labels))
    return data

'''
# TODO: multidimensional generation
it should randomly generate N distribution types, where N is NUM_DIMENSIONS
the function should call the randomly chosen distribution functions from distributions.py, and make an array of N-tuples out of the result
it should also make an array of labels, which is the concatenation of the 1hot, mean and stddev of each distribution
return a pair of those arrays

e.g., if NUM_DIMENSIONS is 2 and SAMPLE_SIZE is 3, we randomly choose 3 dimensions (say normal and gamma)
the function should return something that looks like:
[
   [n1, n2, n3, g1, g2, g3],
   [0,0,0,0,0,0,1,0,0,normal_mean,normal_stddev,0,1,0,0,0,0,0,0,0,gamma_mean,gamma_stddev]
]
where n1,g1 are the first points generated by the normal dist and gamma dist, etc

count is the number of examples to generate
sample_size is the number of points to generate for each distribution
'''

def generate_multidim_data(dimensions, count, sample_size):
    data = []

    for _ in range(count):
        points = []
        labels = []
        items = list(DISTRIBUTIONS.items())
        for _ in range(dimensions):
            dist_name = np.random.choice(list(DISTRIBUTIONS.keys()))
            logger.debug(f'dist_name: {dist_name}')

            # dist_class = np.random.choice(DISTRIBUTIONS.items())
            dist_class = DISTRIBUTIONS[dist_name]
            # Instantiate the class!!! 
            dist_object = dist_class()

            dist_points = dist_object.rng(sample_size)
            logger.debug(f'dist_points from this dist: {dist_points}')
            dist_points = list(dist_points)
            logger.debug(f'dist_points but cast to list: {dist_points}')
            points.append(dist_points)

            labels = dist_object.get_label()
            logger.debug(f'labels from this dist: {labels}')
            labels.extend(labels)

        # List of each dimension's points: [[1,3,5], [2,4,6]]
        logger.debug(f"Each dimension's points: {points}")

        # Flattened: [1,3,5, 2,4,6]
        # The actual points in the 2D distribution will be (1,2), (3,4), (5,6)
        points = [item for dim in points for item in dim]
        logger.debug(f"Flattened: {points}")

        data.append((points, labels))

    return data

def make_dataset(config, mode):
    start = time.time()
    
    # Get the correct size
    if mode == 'train':
        examples_count = config['TRAINING_SIZE']
    elif mode == 'test':
        examples_count = config['TEST_SIZE']

    filename = make_filename(mode,
                            config['TRAINING_SIZE'],
                            config['SAMPLE_SIZE'],
                            NUM_DIMENSIONS)

    # raw_data = generate_data(count=examples_count,
    #                          sample_size=CONFIG['SAMPLE_SIZE'])

    raw_data = generate_multidim_data(dimensions=NUM_DIMENSIONS,
                                      count=examples_count,
                                      sample_size=config['SAMPLE_SIZE'])

    samples = np.array([elem[0] for elem in raw_data])
    labels = np.array([elem[1] for elem in raw_data])
    dataset = MyDataset(samples, labels)
    file_path = os.path.join("data", filename)
    torch.save(dataset, file_path)

    end = time.time()
    logger.info(f"Generated and saved {examples_count} examples out "
                f"to {file_path} in {end - start:.2f} seconds "
                f"(BATCH_SIZE={config['BATCH_SIZE']})")

    return dataset

def get_dataloader(config, filename, mode):
    #TODO: CHANGE HOW FILENAME GETS PASSED IN
    file_path = os.path.join("data", filename)
    if os.path.exists(file_path):
        dataset = torch.load(filename)
    else:
        logger.info(f'Generating fresh data...')
        dataset = make_dataset(config, mode)


    ## TODO: refactor this.
    # try:
    #     dataset = torch.load(filename)

    #     if len(dataset) > examples_count:
    #         dataset = dataset[:examples_count]

    #     # If examples_count is passed in, then adhere to it. Otherwise,
    #     # examples_count is default value, indicating dataset can have any size
    #     acceptable_count = (len(dataset) == examples_count \
    #                         or examples_count == -1)

    #     dataset_sample_size = len(dataset.__getitem__(0)[0])
    #     acceptable_sample_size = (config['SAMPLE_SIZE'] * NUM_DIMENSIONS == dataset_sample_size)

    #     if not isinstance(dataset, Dataset):
    #         raise Exception(f'Could not read dataset from {filename}')
    #     if not acceptable_count:
    #         raise Exception(f"Incorrect dataset size: {len(dataset)}")
    #     if not acceptable_sample_size:
    #         raise Exception(f"Incorrect sample size: model expects "
    #                         f"{config['SAMPLE_SIZE']} points as input, but "
    #                         f"this dataset would give {dataset_sample_size}")

    #     logger.info(f"Using dataset from {filename} (size: {len(dataset)})")

    # except Exception as e:
    #     logger.info(e)
    #     logger.info(f'Generating fresh data...')
    #     dataset = make_dataset(filename, examples_count)

    # If no filename is passed in, the file does not exist, or the file's
    # contents do not represent a DataLoader as expected, then generate some
    # new data, and write it out to the given filename
    dataloader = DataLoader(dataset, batch_size=config['BATCH_SIZE'])
    return dataloader

# If running this file directly as a script, then generate some training
# examples and save them to a file for later use
if __name__ == "__main__":
    data_directory = 'data'
    os.makedirs(data_directory, exist_ok=True)
    make_dataset(CONFIG,mode='train')
    make_dataset(CONFIG,mode='test')
